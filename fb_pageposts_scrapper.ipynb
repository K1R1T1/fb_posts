{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping posts data from Facebook Page for analysis\n",
    "\n",
    "We use Facebook graph API v2.11 in this program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python libraries\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing or Pulling the data from Facebook using Graph API requires an access token, and instructions for creation of access token can be found [here](https://developers.facebook.com/docs/facebook-login/access-tokens#apptokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access_token \n",
    "app_id = 'your_ap_id_here'\n",
    "app_secret = 'your_secret_here'\n",
    "access_token = app_id + '|' + app_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do our analysis with Rolls Royce page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_name = 'rollsroycemotorcars'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to obtain the Facebook Page ID for pulling in the data using FB Graph API (v 2.11)\n",
    "Let us define a function to obtain the fb page id, using page name. \n",
    "\n",
    "The following function is used to gather the facebook page id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpageid (page_name):\n",
    "    base = \"https://graph.facebook.com/v2.11/\"\n",
    "    queryurl = base + page_name + '/?access_token=' + access_token    \n",
    "    page_info = requests.get(queryurl).json()\n",
    "    return page_info['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_id = getpageid(page_name)\n",
    "#print(page_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to obtain the posts of a facebook page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbposts(fburl):\n",
    "    fb_page_posts = req_until_success(fburl).json()\n",
    "    page_posts = fb_page_posts['data']\n",
    "    \n",
    "    end_file = fb_page_posts['paging']\n",
    "    \n",
    "    if 'paging' in fb_page_posts.keys():\n",
    "        end_file = fb_page_posts['paging']\n",
    "        next_url = str(end_file['next']).replace('%2C', ',').replace('%28', '(').replace('%29', ')')\n",
    "    else:\n",
    "        next_url = 'End'\n",
    "    return page_posts, next_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: Extraction of Required Data\n",
    "def postextract(posts_data):\n",
    "    a = {}\n",
    "    for i in range (len(posts_data)):\n",
    "        post_id = '' if 'id' not in posts_data[i].keys() else posts_data[i]['id']\n",
    "        link = '' if 'link' not in posts_data[i].keys() else posts_data[i]['link']\n",
    "        message  = 'NA' if 'message' not in posts_data[i].keys() else posts_data[i]['message']\n",
    "        perma_link = '' if 'permalink_url' not in posts_data[i].keys() else posts_data[i]['permalink_url']\n",
    "        post_type = '' if 'type' not in posts_data[i].keys() else posts_data[i]['type']\n",
    "        if 'created_time' in posts_data[i].keys():\n",
    "            time_info = posts_data[i]['created_time']\n",
    "            time_stamp = time_info.split('T')[1].split('+0000')[0]\n",
    "            date_stamp = time_info.split('T')[0]\n",
    "        else:\n",
    "            time_stamp, date_stamp = \"NA\", \"NA\"\n",
    "        comments = '' if 'comments' not in posts_data[i].keys() else posts_data[i]['comments']['summary']['total_count']\n",
    "        shares = '' if 'shares' not in posts_data[i].keys() else posts_data[i]['shares']['count']\n",
    "        likes = '' if 'likes' not in posts_data[i].keys() else posts_data[i]['likes']['summary']['total_count']\n",
    "        reactions_none = '' if 'reactions_none' not in posts_data[i].keys() else posts_data[i]['reactions_none']['summary']['total_count']\n",
    "        reactions_love = '' if 'reactions_love' not in posts_data[i].keys() else posts_data[i]['reactions_love']['summary']['total_count']\n",
    "        reactions_wow = '' if 'reactions_wow' not in posts_data[i].keys() else posts_data[i]['reactions_wow']['summary']['total_count']\n",
    "        reactions_haha = '' if 'reactions_haha' not in posts_data[i].keys() else posts_data[i]['reactions_haha']['summary']['total_count']        \n",
    "        reactions_sad = '' if 'reactions_sad' not in posts_data[i].keys() else posts_data[i]['reactions_sad']['summary']['total_count']\n",
    "        reactions_angry = '' if 'reactions_angry' not in posts_data[i].keys() else posts_data[i]['reactions_angry']['summary']['total_count']\n",
    "        a[i] = [date_stamp, time_stamp, post_id, perma_link, link, message, post_type,   \n",
    "                 comments, shares, reactions_none, likes, reactions_love, \n",
    "                 reactions_wow, reactions_haha, reactions_sad, reactions_angry]\n",
    "    return a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV Writer\n",
    "#This function writes all the extracted posts into CSV file.  \n",
    "def tocsv(name,posts_list):\n",
    "    outputfile = dirname + \"/\" + page_name + '_' + 'posts' + name + '.csv'\n",
    "    print('working on {}'.format(name))\n",
    "    with open(outputfile, 'w', encoding = 'utf-8-sig', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        Header = (\"Date\", \"Time\", \"ID\" , \"PermaLink\" , \"Link\", \"Message\", \"Post Type\", \n",
    "                        \"Comments\", \"Shares\", \"Total Reactions\", \"Likes\",\n",
    "                        \"Love\",\"Wow\",\"Haha\", \"Sad\",\"Angry\")\n",
    "        writer.writerow(Header)\n",
    "        for k in posts_list.keys():\n",
    "            writer.writerow(posts_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function\n",
    "param = '/posts/?fields=message,link,permalink_url,created_time,type,name,id,comments.limit(0).summary(true),shares,likes.limit(0).summary(true),reactions.type(NONE).limit(0).summary(total_count).as(reactions_none),reactions.type(LIKE).limit(0).summary(total_count).as(reactions_like),reactions.type(LOVE).limit(0).summary(total_count).as(reactions_love),reactions.type(WOW).limit(0).summary(total_count).as(reactions_wow),reactions.type(HAHA).limit(0).summary(total_count).as(reactions_haha),reactions.type(SAD).limit(0).summary(total_count).as(reactions_sad),reactions.type(ANGRY).limit(0).summary(total_count).as(reactions_angry)&limit=100&access_token='\n",
    "posts_queryurl = base + page_id + param + access_token\n",
    "posts, next_url = fbposts(posts_queryurl)\n",
    "a_posts = postextract(posts)\n",
    "tocsv('Part1', a_posts)\n",
    "z = 1\n",
    "endofposts = False \n",
    "\n",
    "#Iterate and pullin all the posts within the date range\n",
    "if next_url != 'End':\n",
    "    next_url_part_c = next_url\n",
    "    while next_url_part_c != 'End':\n",
    "        z += 1\n",
    "        kl  = \"Part\" + str(z) \n",
    "        next_posts_part,next_url_part = fbposts(next_url_part_c)\n",
    "        b_posts = postextract(next_posts_part)\n",
    "        tocsv(str(kl), b_posts)\n",
    "        next_url_part_c = next_url_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge CSV files\n",
    "to_merge = glob.glob(dirname+\"/*.csv\")\n",
    "to_merge.sort()\n",
    "header_saved = False\n",
    "outputfile_m = dirname + \"/\" + page_name + '_' + 'posts'+ '_' +'merged.csv'\n",
    "with open(outputfile_m,'w', encoding = 'utf-8-sig') as mergedfile:\n",
    "    for filename in to_merge:\n",
    "        with open(filename, encoding = 'utf-8-sig') as infiles:\n",
    "            header = next(infiles)\n",
    "            if not header_saved:\n",
    "                mergedfile.write(header)\n",
    "                header_saved = True\n",
    "            for line in infiles:\n",
    "                mergedfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Posts of page {} from {} to {} are obtained'.format(page_name, start_date,end_date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
